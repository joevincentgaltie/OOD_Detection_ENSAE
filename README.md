
# From simplistic supervised to advances self-supervised OOD Detection

Context : 
This project delves into the issue of Textual Out-Of-Distribution (OOD) detection, which refers to the capability of machine learning models to recognize data samples that significantly deviate from their training data distribution. In Natural Language Processing (NLP) applications, Textual OOD detection is critical to ensuring the robustness and depend- ability of production systems. This study in- vestigates the effectiveness of various meth- ods for OOD detection in NLP, utilizing a transformer-based language model and differ- ent datasets with varying degrees of similarity to the training data. Our findings demonstrate that both the Mahalanobis-based score utiliz- ing the last layer representation and the Co- sine Projection score utilizing the average la- tent representation outperform the other scores in terms of AUROC. However, the supervised approach did not perform as well. 


Writing...

The repositories [Todd](https://github.com/icannos/Todd) and [ToddBenchmark](https://github.com/icannos/ToddBenchmark) enabled us to do this work. 

