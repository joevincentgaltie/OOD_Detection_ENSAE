
# From simplistic supervised to advances self-supervised OOD Detection

Context : 
This project delves into the issue of Textual Out-Of-Distribution (OOD) detection, which refers to the capability of machine learning models to recognize data samples that significantly deviate from their training data distribution. In Natural Language Processing (NLP) applications, Textual OOD detection is critical to ensuring the robustness and depend- ability of production systems. This study investigates the effectiveness of various methods for OOD detection in NLP, utilizing a transformer-based language model and different datasets with varying degrees of similarity to the training data. Our findings demonstrate that both the Mahalanobis-based score utilizing the last layer representation and the Cosine Projection score utilizing the average latent representation outperform the other scores in terms of AUROC. However, the supervised approach did not perform as well. 


Writing...

The repositories [Todd](https://github.com/icannos/Todd) and [ToddBenchmark](https://github.com/icannos/ToddBenchmark) enabled us to do this work. 

